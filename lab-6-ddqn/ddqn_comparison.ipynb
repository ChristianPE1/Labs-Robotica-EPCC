{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421b0b92",
   "metadata": {},
   "source": [
    "# Laboratorio 6: Comparación DQN vs Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f7a25",
   "metadata": {},
   "source": [
    "## 1. Configuración del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348bbc3a",
   "metadata": {},
   "source": [
    "## 2. Clonar Repositorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ChristianPE1/Labs-Robotica-EPCC.git\n",
    "%cd Labs-Robotica-EPCC/lab-6-ddqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d24569f",
   "metadata": {},
   "source": [
    "## 3. Importar Módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ca073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import config\n",
    "from dqn_agent import DQNAgent\n",
    "from utils import (\n",
    "    save_metrics, load_metrics, set_random_seed,\n",
    "    compute_moving_average, evaluate_agent, get_device_info\n",
    ")\n",
    "from visualize import (\n",
    "    plot_reward_comparison, plot_q_value_comparison,\n",
    "    plot_loss_comparison, plot_success_rate_comparison,\n",
    "    plot_convergence_comparison, print_comparison_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42fd3c",
   "metadata": {},
   "source": [
    "## 4. Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bab82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Configuración del Experimento:\")\n",
    "print(f\"  Entorno: {config.ENV_NAME}\")\n",
    "print(f\"  Episodios: {config.NUM_EPISODES}\")\n",
    "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Gamma: {config.GAMMA}\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Memory size: {config.MEMORY_SIZE}\")\n",
    "print(f\"  Hidden layers: {config.HIDDEN_LAYERS}\")\n",
    "print(f\"  Target update freq: {config.TARGET_UPDATE_FREQ}\")\n",
    "print(f\"  Early stop threshold: {config.EARLY_STOP_THRESHOLD}\")\n",
    "print(f\"  Dispositivo: {config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff5548",
   "metadata": {},
   "source": [
    "## 5. Función de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b05ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(use_double_dqn=False, verbose=True):\n",
    "    algorithm_name = \"Double DQN\" if use_double_dqn else \"DQN\"\n",
    "    algorithm_prefix = \"ddqn\" if use_double_dqn else \"dqn\"\n",
    "    \n",
    "    # Configurar semilla\n",
    "    set_random_seed(config.RANDOM_SEED)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Entrenando {algorithm_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    # Crear entorno\n",
    "    env = gym.make(config.ENV_NAME)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    # Inicializar agente\n",
    "    agent = DQNAgent(\n",
    "        state_dim, action_dim, config, config.DEVICE,\n",
    "        use_double_dqn=use_double_dqn\n",
    "    )\n",
    "    \n",
    "    # Métricas\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    episode_losses = []\n",
    "    episode_q_values = []\n",
    "    success_count = 0\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Iniciando entrenamiento...\")\n",
    "    \n",
    "    for episode in range(config.NUM_EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        episode_loss_values = []\n",
    "        episode_q_vals = []\n",
    "        \n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            # Obtener Q-values para análisis\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(config.DEVICE)\n",
    "                q_values = agent.policy_net(state_tensor)\n",
    "                max_q = q_values.max().item()\n",
    "                episode_q_vals.append(max_q)\n",
    "            \n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            agent.store_transition(state, action, reward, next_state, done or truncated)\n",
    "            \n",
    "            loss = agent.train_step()\n",
    "            if loss is not None:\n",
    "                episode_loss_values.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "        \n",
    "        # Registrar métricas\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "        episode_losses.append(np.mean(episode_loss_values) if episode_loss_values else 0.0)\n",
    "        episode_q_values.append(np.mean(episode_q_vals) if episode_q_vals else 0.0)\n",
    "        \n",
    "        if episode_length >= 500:\n",
    "            success_count += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        window = min(100, episode + 1)\n",
    "        avg_reward = np.mean(episode_rewards[-window:])\n",
    "        \n",
    "        if avg_reward >= config.EARLY_STOP_THRESHOLD:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= config.EARLY_STOP_PATIENCE:\n",
    "                if verbose:\n",
    "                    print(f\"\\nEarly stopping en episodio {episode + 1}\")\n",
    "                break\n",
    "        else:\n",
    "            early_stop_counter = 0\n",
    "        \n",
    "        # Imprimir progreso\n",
    "        if verbose and (episode + 1) % 50 == 0:\n",
    "            success_rate = (success_count / (episode + 1)) * 100\n",
    "            print(f\"Ep {episode+1}/{config.NUM_EPISODES} | \"\n",
    "                  f\"Reward: {episode_reward:.0f} | \"\n",
    "                  f\"Avg: {avg_reward:.1f} | \"\n",
    "                  f\"Success: {success_rate:.1f}%\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Guardar métricas\n",
    "    metrics = {\n",
    "        'algorithm': algorithm_name,\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'episode_losses': episode_losses,\n",
    "        'episode_q_values': episode_q_values,\n",
    "        'total_episodes': len(episode_rewards),\n",
    "        'success_count': success_count,\n",
    "        'final_epsilon': agent.epsilon\n",
    "    }\n",
    "    \n",
    "    os.makedirs('metrics', exist_ok=True)\n",
    "    save_metrics(metrics, f'metrics/{algorithm_prefix}_metrics.pkl')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{algorithm_name} completado!\")\n",
    "        print(f\"Episodios: {len(episode_rewards)}\")\n",
    "        print(f\"Recompensa promedio: {np.mean(episode_rewards):.2f}\")\n",
    "        print(f\"Tasa de éxito: {(success_count/len(episode_rewards))*100:.2f}%\")\n",
    "    \n",
    "    return metrics, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb7a67",
   "metadata": {},
   "source": [
    "## 6. Entrenar DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_metrics, dqn_agent = train_agent(use_double_dqn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49202be",
   "metadata": {},
   "source": [
    "## 7. Entrenar Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa46cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddqn_metrics, ddqn_agent = train_agent(use_double_dqn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf6209c",
   "metadata": {},
   "source": [
    "## 8. Comparación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3979b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_comparison_summary(dqn_metrics, ddqn_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d68344",
   "metadata": {},
   "source": [
    "## 9. Visualizaciones Comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1290518",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# Generar todas las gráficas\n",
    "plot_reward_comparison(dqn_metrics, ddqn_metrics)\n",
    "plot_q_value_comparison(dqn_metrics, ddqn_metrics)\n",
    "plot_loss_comparison(dqn_metrics, ddqn_metrics)\n",
    "plot_success_rate_comparison(dqn_metrics, ddqn_metrics)\n",
    "plot_convergence_comparison(dqn_metrics, ddqn_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71566387",
   "metadata": {},
   "source": [
    "### 9.1 Comparación de Recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6874507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image('plots/reward_comparison.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c6f8c",
   "metadata": {},
   "source": [
    "### 9.2 Análisis de Q-Values (Sobreestimación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a57cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image('plots/q_value_comparison.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb00e74",
   "metadata": {},
   "source": [
    "### 9.3 Comparación de Pérdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ba590",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image('plots/loss_comparison.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45afc7",
   "metadata": {},
   "source": [
    "### 9.4 Tasa de Éxito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64492994",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image('plots/success_rate_comparison.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196be38",
   "metadata": {},
   "source": [
    "### 9.5 Análisis de Convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image('plots/convergence_comparison.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd35c65",
   "metadata": {},
   "source": [
    "## 10. Evaluación de Agentes Entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar DQN\n",
    "env = gym.make(config.ENV_NAME)\n",
    "dqn_eval = evaluate_agent(dqn_agent, env, num_episodes=20)\n",
    "env.close()\n",
    "\n",
    "print(\"Evaluación DQN (20 episodios):\")\n",
    "print(f\"  Recompensa promedio: {dqn_eval['mean_reward']:.2f} ± {dqn_eval['std_reward']:.2f}\")\n",
    "print(f\"  Longitud promedio: {dqn_eval['mean_length']:.1f}\")\n",
    "print(f\"  Tasa de éxito: {dqn_eval['success_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b4a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar Double DQN\n",
    "env = gym.make(config.ENV_NAME)\n",
    "ddqn_eval = evaluate_agent(ddqn_agent, env, num_episodes=20)\n",
    "env.close()\n",
    "\n",
    "print(\"Evaluación Double DQN (20 episodios):\")\n",
    "print(f\"  Recompensa promedio: {ddqn_eval['mean_reward']:.2f} ± {ddqn_eval['std_reward']:.2f}\")\n",
    "print(f\"  Longitud promedio: {ddqn_eval['mean_length']:.1f}\")\n",
    "print(f\"  Tasa de éxito: {ddqn_eval['success_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7de295",
   "metadata": {},
   "source": [
    "## 11. Análisis de Sobreestimación de Q-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ace98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar Q-values promedio\n",
    "dqn_q_mean = np.mean(dqn_metrics['episode_q_values'])\n",
    "ddqn_q_mean = np.mean(ddqn_metrics['episode_q_values'])\n",
    "\n",
    "print(\"Análisis de Sobreestimación de Q-Values:\")\n",
    "print(f\"\\n  DQN Q-value promedio: {dqn_q_mean:.2f}\")\n",
    "print(f\"  Double DQN Q-value promedio: {ddqn_q_mean:.2f}\")\n",
    "print(f\"\\n  Diferencia: {dqn_q_mean - ddqn_q_mean:.2f}\")\n",
    "\n",
    "if dqn_q_mean > ddqn_q_mean:\n",
    "    reduction = ((dqn_q_mean - ddqn_q_mean) / dqn_q_mean) * 100\n",
    "    print(f\"  Double DQN reduce sobreestimación en {reduction:.1f}%\")\n",
    "else:\n",
    "    print(\"  No se observa reducción de sobreestimación en este experimento\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
