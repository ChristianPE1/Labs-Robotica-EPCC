{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff2c211",
   "metadata": {},
   "source": [
    "# Deep Q-Network Training with CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239274a",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffae83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch gymnasium numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b76b8b",
   "metadata": {},
   "source": [
    "## Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a085ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ChristianPE1/Labs-Robotica-EPCC.git\n",
    "%cd Labs-Robotica-EPCC/lab-5-webots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1703b",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import config\n",
    "from dqn_agent import DQNAgent\n",
    "from utils import get_device_info, compute_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf91a987",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DQN Configuration:\")\n",
    "print(f\"  Environment: {config.ENV_NAME}\")\n",
    "print(f\"  Episodes: {config.NUM_EPISODES}\")\n",
    "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Gamma: {config.GAMMA}\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Memory size: {config.MEMORY_SIZE}\")\n",
    "print(f\"  Hidden layers: {config.HIDDEN_LAYERS}\")\n",
    "print(f\"  Device: {config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c9154",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edaa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e019f6b",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python visualize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8aecc",
   "metadata": {},
   "source": [
    "## Load and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80044d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_metrics\n",
    "\n",
    "metrics = load_metrics('metrics/training_metrics.pkl')\n",
    "\n",
    "episode_rewards = metrics['episode_rewards']\n",
    "episode_lengths = metrics['episode_lengths']\n",
    "\n",
    "print(\"Training Statistics:\")\n",
    "print(f\"  Total episodes: {len(episode_rewards)}\")\n",
    "print(f\"  Average reward: {np.mean(episode_rewards):.2f}\")\n",
    "print(f\"  Max reward: {np.max(episode_rewards):.2f}\")\n",
    "print(f\"  Average episode length: {np.mean(episode_lengths):.1f}\")\n",
    "print(f\"  Max episode length: {np.max(episode_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaac86b",
   "metadata": {},
   "source": [
    "## Display Generated Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed5f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"Reward Curve:\")\n",
    "display(Image('plots/reward_curve.png'))\n",
    "\n",
    "print(\"\\nEpisode Length:\")\n",
    "display(Image('plots/episode_length.png'))\n",
    "\n",
    "print(\"\\nLoss Curve:\")\n",
    "display(Image('plots/loss_curve.png'))\n",
    "\n",
    "print(\"\\nSuccess Rate:\")\n",
    "display(Image('plots/success_rate.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cf394f",
   "metadata": {},
   "source": [
    "## Test Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e84676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_checkpoint\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(config.ENV_NAME, render_mode='rgb_array')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Initialize agent\n",
    "agent = DQNAgent(state_dim, action_dim, config, config.DEVICE)\n",
    "\n",
    "# Load trained model\n",
    "checkpoint = torch.load('checkpoints/dqn_final.pt')\n",
    "agent.load_state_dict(checkpoint['agent_state'])\n",
    "\n",
    "# Test for 10 episodes\n",
    "test_rewards = []\n",
    "test_lengths = []\n",
    "\n",
    "for episode in range(10):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        action = agent.select_action(state, training=False)\n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "    \n",
    "    test_rewards.append(episode_reward)\n",
    "    test_lengths.append(episode_length)\n",
    "    print(f\"Test Episode {episode + 1}: Reward = {episode_reward:.2f}, Length = {episode_length}\")\n",
    "\n",
    "print(f\"\\nTest Statistics:\")\n",
    "print(f\"  Average reward: {np.mean(test_rewards):.2f}\")\n",
    "print(f\"  Average length: {np.mean(test_lengths):.1f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b9c0e",
   "metadata": {},
   "source": [
    "## Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9da445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('checkpoints/dqn_final.pt')\n",
    "\n",
    "files.download('metrics/training_metrics.pkl')\n",
    "\n",
    "files.download('plots/reward_curve.png')\n",
    "files.download('plots/episode_length.png')\n",
    "files.download('plots/loss_curve.png')\n",
    "files.download('plots/success_rate.png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
