# CONFIGURACIÓN DE DQN (Deep Q-Network)

# HIPERPARÁMETROS DQN
LEARNING_RATE = 0.001  # Tasa de aprendizaje para la red neuronal
GAMMA = 0.99           # Factor de descuento

# Exploración vs Explotación
EPSILON_START = 0.5    # Exploración inicial
EPSILON_END = 0.01     # Exploración mínima
EPSILON_DECAY = 0.995  # Decaimiento

# Replay Buffer
REPLAY_BUFFER_SIZE = 2000
BATCH_SIZE = 64
MIN_REPLAY_SIZE = 200

# Target Network
TARGET_UPDATE_FREQUENCY = 10

# PARÁMETROS DEL ROBOT
MAX_SPEED = 6.28  # Velocidad máxima del E-puck (rad/s)
TIME_STEP = 64

# ARQUITECTURA DE LA RED
INPUT_SIZE = 12      # 8 sensores + 2 LIDAR + 2 posición relativa a meta
HIDDEN_LAYERS = [32] # Una capa oculta de 32 neuronas
OUTPUT_SIZE = 8      # 8 acciones posibles

# ENTRENAMIENTO
EPISODES = 500
MAX_STEPS_PER_EPISODE = 200

# SISTEMA DE RECOMPENSAS
REWARD_GOAL = 200.0
REWARD_OPEN_SPACE = 1.0
PENALTY_OBSTACLE_VERY_CLOSE = -10.0
PENALTY_OBSTACLE_CLOSE = -5.0
PENALTY_OBSTACLE_NEAR = -2.0
PENALTY_STEP = -0.1

# DETECCIÓN DE OBSTÁCULOS
SENSOR_THRESHOLD_VERY_CLOSE = 800
SENSOR_THRESHOLD_CLOSE = 400
SENSOR_THRESHOLD_NEAR = 150

# POSICIONES DEL MUNDO
START_POS = [-1.2, -1.2, 0.01]
GOAL_POS = [1.2, 1.2]
GOAL_TOLERANCE = 0.3

# DETECCIÓN DE META
LIDAR_GOAL_THRESHOLD = 1.0
LIDAR_MIN_CLEAR_RATIO = 0.80

# ACCIONES DEL ROBOT
ACTIONS = {
    0: [0.8, 0.8],   # FORWARD
    1: [-0.3, 0.8],  # TURN_LEFT
    2: [0.8, -0.3],  # TURN_RIGHT
    3: [-0.6, 0.6],  # SHARP_LEFT
    4: [0.6, -0.6],  # SHARP_RIGHT
    5: [0.0, 0.0],   # STOP
    6: [0.4, 0.8],   # SLIGHT_LEFT
    7: [0.8, 0.4],   # SLIGHT_RIGHT
}

ACTION_NAMES = ['FORWARD', 'TURN_LEFT', 'TURN_RIGHT', 'SHARP_LEFT', 'SHARP_RIGHT', 'STOP', 'SLIGHT_LEFT', 'SLIGHT_RIGHT']

# GUARDADO Y CARGA
MODEL_SAVE_PATH = 'dqn_model.pth'
CHECKPOINT_FREQUENCY = 50
SAVE_BEST_MODEL = True

# LOGGING Y DEBUG
LOG_FREQUENCY = 10
SAVE_TRAINING_LOG = True
LOG_FILENAME = 'dqn_training_log.txt'
PLOT_FREQUENCY = 10

# DEVICE
USE_GPU = False
DEVICE = "cuda" if USE_GPU else "cpu"
